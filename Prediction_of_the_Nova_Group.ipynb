{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction_of_the_Nova_Group.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aziz1998-lemriss/OpenFoodFacts/blob/main/Prediction_of_the_Nova_Group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8d3BHiX29PD"
      },
      "source": [
        "# Project Descreption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB7qY8bs26BD"
      },
      "source": [
        "The Objective of this project is :\n",
        "\n",
        "1.   Use the OpenFoodFact   that shows  characteristics of certain product groups, similarities between products and product groups, to provide a global view of the dataset, and exhibit salient features that are of interest for an analyst or stakeholder in this sector.\n",
        "\n",
        "2.   Use some machine learning algorithms to : \n",
        "\n",
        "        *   predict the **nutriscore_grade** of a product given nutritional values and possibly other fields (as few as possible)\n",
        "        *   predict the **nova_group** of a product given nutritional values and possibly other fields (as few as possible),\n",
        "        *   predict the **pnns_groups_1** of a product given nutritional values and possibly other fields (as few as possible),\n",
        "        *   predict the **pnns_groups_2** of a product given nutritional values and possibly other fields (as few as possible),\n",
        "        *   predict the **categories** (either atomic categories or lists of categories) of a product given nutritional values and possibly other fields (as few as possible),\n",
        "        *   predict one or more **nutritional values** (ex: sugars_100g) given nutritional values and possibly other fields (as few as possible)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIzZtUkU7hHC"
      },
      "source": [
        "# Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfs3MJGW0Nef"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn')\n",
        "from scipy.stats import norm, skew\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import  StandardScaler,  LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from scipy.stats import skew\n",
        "from scipy.special import boxcox1p\n",
        "from scipy.stats import boxcox_normmax\n",
        "import statsmodels.api as sm\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, plot_roc_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3sSjVaZ8ZJ9"
      },
      "source": [
        "# Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJoQnJ0X1PnT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nygdy2reyHCO"
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/off_complete.csv', sep = '\\t', nrows = 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtizgEgULffV"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssQyBMqNyKLs"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vdBaJTN8fRM"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amZQmAOppN1j"
      },
      "source": [
        "data.drop(['salt_100g'], 1, inplace=True)\n",
        "data.drop(['nutrition-score-fr_100g'], 1, inplace=True)                         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFpcCw04739T"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl8C-Sq28qDd"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjoUW3UZ9s_k"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWClt78lIisy"
      },
      "source": [
        "**Note**\n",
        "\n",
        "While the code of products refers to manufacturer-specific coding with tariff indication, The country or the company markets the product, The manufacturer, the Article code and a control key, so we can drop the  product code , because we already have the country that markets the product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V8WxKwb-n4k"
      },
      "source": [
        "data.drop(['code'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua4oj2IhKwc_"
      },
      "source": [
        "data['url'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIxy0SMIK7oj"
      },
      "source": [
        "**Note :**\n",
        "\n",
        "We see that in every url of the products, it contains the product name and the brand, so we can drop this feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAtJXivsKwbz"
      },
      "source": [
        "data.drop(['url'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY-o4eXSvagS"
      },
      "source": [
        "I will Try to find the pourcentage of the null values in every features, and drop the features with more than 70% of null values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7B8G43GOBQB"
      },
      "source": [
        "for i in data.columns:\n",
        "    h = (data[i].isnull().sum()/len(data['product_name']))*100\n",
        "    print('The pourcentage of the null values of '+i+' is : ', h, '%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHZMyhhKvxt7"
      },
      "source": [
        "data.drop(['trans-fat_100g'], 1, inplace=True)\n",
        "data.drop(['cholesterol_100g'], 1, inplace=True)\n",
        "data.drop(['vitamin-a_100g'], 1, inplace=True)\n",
        "data.drop(['vitamin-c_100g'], 1, inplace=True)\n",
        "data.drop(['calcium_100g'], 1, inplace=True)\n",
        "data.drop(['iron_100g'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaG-UebI3JSr"
      },
      "source": [
        "I will Pick up the countries_tags with highest number of products \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MeQRroKHauj"
      },
      "source": [
        "data['countries_tags'].value_counts().head(17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xV3okyBLPXg"
      },
      "source": [
        "data['countries_tags']=data['countries_tags'].fillna('Unknown')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pwIDV8TKoiU"
      },
      "source": [
        "countries_tags_values = data['countries_tags'].values\n",
        "most_countries = ('en:france' , 'en:germany',  'en:spain' , 'en:mexico', 'en:united-kingdom' ,'en:canada', 'en:united-states', 'en:belgium','en:switzerland',  'en:poland')\n",
        "countrie= {'en:france' : [], 'en:germany': [],  'en:spain': [] , 'en:mexico': [], 'en:united-kingdom': [], 'en:canada': [], 'en:united-states': [], 'en:belgium': [],\n",
        "           'en:switzerland': [],'en:poland': []} \n",
        "for i in countries_tags_values : \n",
        "   for j in most_countries:\n",
        "       if j in str(i):\n",
        "         countrie[j].append(1)\n",
        "       else : \n",
        "          countrie[j].append(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1_kQisd5F9I"
      },
      "source": [
        "for i in most_countries:\n",
        "  data[i] = pd.DataFrame(countrie[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wZ9RPDKRLz2"
      },
      "source": [
        "data.drop(['countries_tags'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr4_sYzzaBzl"
      },
      "source": [
        "I will work on the Product Name Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngFT112cXB_U"
      },
      "source": [
        "product_names_list = data['product_name'].values\n",
        "list_products = []\n",
        "for i in product_names_list:\n",
        "  i = i.replace('Le', '')\n",
        "  i = i.replace('La', '')\n",
        "  i = i.replace('2', '')\n",
        "  first_word = i.split()[0]\n",
        "  list_products.append(first_word)\n",
        "data['product_name'] = pd.DataFrame(list_products)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMzZeWFvXB9J"
      },
      "source": [
        "data['product_name'].value_counts().head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg8X-36TZHDf"
      },
      "source": [
        "product_name__values = data['product_name'].values\n",
        "most_products = ('Organic' , 'Pain',  'Chocolate' , 'Filet', 'Sauce' ,'Salade', 'Crème', 'Jambon','Original', 'Bio', 'Jus')\n",
        "products= {'Organic':[] , 'Pain':[],  'Chocolate':[] , 'Filet':[], 'Sauce':[] ,'Salade':[], 'Crème':[], 'Jambon':[],'Original':[], 'Bio':[], 'Jus':[]} \n",
        "for i in product_name__values : \n",
        "   for j in most_products:\n",
        "       if j in str(i):\n",
        "         products[j].append(1)\n",
        "       else : \n",
        "          products[j].append(0)\n",
        "for i in most_products:\n",
        "  data[i] = pd.DataFrame(products[i])\n",
        "data.drop(['product_name'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAERKUZfdD0_"
      },
      "source": [
        "data.drop(['brands'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxI2o77SdP32"
      },
      "source": [
        "categories_list = data['categories'].values\n",
        "list_categories = []\n",
        "for i in categories_list:\n",
        "  first_word = i.split()[0]\n",
        "  first_word = first_word.replace(',', '')\n",
        "  list_categories.append(first_word)\n",
        "data['categories'] = pd.DataFrame(list_categories)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_MLVY74dv-E"
      },
      "source": [
        "data['categories'].value_counts().head(11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGzkq-5Rd-Uy"
      },
      "source": [
        "categories__values = data['categories'].values\n",
        "most_categories = ('Snacks' , 'Aliments',  'Plant-based' , 'Produits', 'Alimentos' ,'Viandes', 'Boissons', 'Pflanzliche','Plats', 'Groceries', 'Dairies')\n",
        "categories= {'Snacks':[] , 'Aliments':[],  'Plant-based':[] , 'Produits':[], 'Alimentos':[] ,'Viandes':[], 'Boissons':[], 'Pflanzliche':[],'Plats':[], 'Groceries':[], 'Dairies':[]} \n",
        "for i in categories__values : \n",
        "   for j in most_categories:\n",
        "       if j in str(i):\n",
        "         categories[j].append(1)\n",
        "       else : \n",
        "          categories[j].append(0)\n",
        "for i in most_categories:\n",
        "  data[i] = pd.DataFrame(categories[i])\n",
        "data.drop(['categories'], 1, inplace=True)\n",
        "data['Aliments'] = data['Aliments']+ data['Alimentos']\n",
        "aliments_values = data['Aliments'].values\n",
        "for i in range(len(aliments_values)):\n",
        "  if aliments_values[i]==2:\n",
        "        aliments_values[i] = 1\n",
        "data['Aliments'] = pd.DataFrame(aliments_values)\n",
        "data.drop(['Alimentos'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEm8lfVvdP2y"
      },
      "source": [
        "data.drop(['additives_tags','states'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djyt0LCDRVji"
      },
      "source": [
        "data['nutriscore_grade'].fillna('None', inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oft5O3asKqPD"
      },
      "source": [
        "def function_escalier(x):\n",
        "  if x >=0:\n",
        "      return(int(x+0.5))\n",
        "  else:\n",
        "    return int(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn2v_vvtkMez"
      },
      "source": [
        "data['nutriscore_score'] = data['nutriscore_score'].astype(float)\n",
        "data_grouped = data.groupby('pnns_groups_2')['nutriscore_score'].mean()\n",
        "grades = list(data['pnns_groups_2'].unique())\n",
        "for grade in grades:\n",
        "  p = data['pnns_groups_2'] == grade\n",
        "  data.loc[p, 'nutriscore_score'] = data.loc[p, 'nutriscore_score'].fillna(function_escalier(data_grouped[grade]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SktuL5wgWBf-"
      },
      "source": [
        "dh = data.groupby(['pnns_groups_1'])['pnns_groups_2'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEMP5GvKZJDm"
      },
      "source": [
        "dh = list(dict(dh))\n",
        "dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWZSeiKYbGjn"
      },
      "source": [
        "data['pnns_groups_1'].fillna('To Replace', inplace = True)\n",
        "d = data[data['pnns_groups_1'] =='To Replace']['pnns_groups_2']\n",
        "d.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIgvOHpgthyj"
      },
      "source": [
        "new_data = data[['pnns_groups_1','pnns_groups_2']].values\n",
        "a_list1 = []\n",
        "a_list2 = []\n",
        "for i in range(len(new_data)):\n",
        "     if new_data[i][1]=='Alcoholic beverages':\n",
        "        new_data[i][0] ='Beverages'\n",
        "     if new_data[i][1]=='Pizza pies and quiches':\n",
        "        new_data[i][0] ='Composite foods'\n",
        "df= pd.DataFrame(new_data, columns = ['pnns_groups_1','pnns_groups_2'])\n",
        "data['pnns_groups_1'] = df['pnns_groups_1'] \n",
        "data['pnns_groups_2'] = df['pnns_groups_2'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNZly8eNpRP2"
      },
      "source": [
        "lists= ['energy-kcal_100g','fat_100g', 'saturated-fat_100g','carbohydrates_100g','sugars_100g','fiber_100g','proteins_100g','sodium_100g']\n",
        "for feature in lists:\n",
        "    data[feature] = data[feature].astype(float)\n",
        "    data_grouped = data.groupby('pnns_groups_2')[feature].mean()\n",
        "    list_feature = list(data['pnns_groups_2'].unique())\n",
        "    for value_f in list_feature:\n",
        "         p = data['pnns_groups_2'] == value_f\n",
        "         data.loc[p, feature] = data.loc[p, feature].fillna(data_grouped[value_f])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9dfMu8PSh3f"
      },
      "source": [
        "data['nova_group'].fillna(0, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7njCzbItxueU"
      },
      "source": [
        "data['fat_100g'] = data['fat_100g'].astype(float)\n",
        "data_grouped = data.groupby('pnns_groups_2')['fat_100g'].mean()\n",
        "grades = list(data['pnns_groups_2'].unique())\n",
        "for grade in grades:\n",
        "  p = data['pnns_groups_2'] == grade\n",
        "  data.loc[p, 'fat_100g'] = data.loc[p, 'fat_100g'].fillna(function_escalier(data_grouped[grade]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UooUkZNOnfH"
      },
      "source": [
        "df =pd.get_dummies(data['nutriscore_grade'], drop_first=True)\n",
        "for i in df.columns : \n",
        "         data['nutriscore_grade_'+i] = df[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mu3wCltb5Jm"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvrwM7Xzb5D8"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syqGOcGbtDEq"
      },
      "source": [
        "data['pnns_groups_2'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zhpv6WKwWRu"
      },
      "source": [
        "df =pd.get_dummies(data[['pnns_groups_2']], drop_first=True)\n",
        "for i in df.columns : \n",
        "         data[i] = df[i]\n",
        "data.drop(['pnns_groups_2'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCyUrxNNwWWh"
      },
      "source": [
        "df =pd.get_dummies(data[['pnns_groups_1']], drop_first=True)\n",
        "for i in df.columns : \n",
        "         data[i] = df[i]\n",
        "data.drop(['pnns_groups_1'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvUzhBNonhWp"
      },
      "source": [
        "data['nutriscore_score'] = data['nutriscore_score'].astype(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bltxVA5Tqok"
      },
      "source": [
        "data.drop(['nutriscore_grade'], 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcKum8QIwWVO"
      },
      "source": [
        "data.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpVsh0rR0QFS"
      },
      "source": [
        "features = data\n",
        "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "numerics2 = []\n",
        "for i in features.columns:\n",
        "    if features[i].dtype in numeric_dtypes: \n",
        "        numerics2.append(i)\n",
        "\n",
        "skew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
        "skews = pd.DataFrame({'skew':skew_features})\n",
        "skews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRgqM_bt0RBp"
      },
      "source": [
        "**Some Statistics : Skewness**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj7KVqqK0QT2"
      },
      "source": [
        "**Handling The  Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEyIMYLZ2vaC"
      },
      "source": [
        "tr = data\n",
        "for y in tr.columns :\n",
        "  factor = 4\n",
        "  upper_lim = data[y].mean () + data[y].std () * factor\n",
        "  lower_lim = data[y].mean () - data[y].std () * factor\n",
        "  tr = data[(data[y] < upper_lim) & (data[y] > lower_lim)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os6UQSXFKM6G"
      },
      "source": [
        "\n",
        "# Prediction of the nova_group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH-0jbL33qpI"
      },
      "source": [
        "X = data.drop(['nova_group'], 1)\n",
        "y = data['nova_group']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noc-Qhi43epv"
      },
      "source": [
        "overfit = []\n",
        "for i in X.columns:\n",
        "    counts = X[i].value_counts()\n",
        "    zeros = counts.iloc[0]\n",
        "    if zeros / len(X) * 100 >99.94:\n",
        "        overfit.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-1ATJSQ44WA"
      },
      "source": [
        "overfit = list(overfit)\n",
        "overfit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn5xuoQh5MEu"
      },
      "source": [
        "Let's drop these overfits from 'X' .  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtI00pJa5YHE"
      },
      "source": [
        "X.drop(overfit,axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF-IO5EZnUZW"
      },
      "source": [
        "Spliting the datasets into the training set and the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k837xwjD5lsh"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNBYoxaAnbPl"
      },
      "source": [
        "Scaling the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfQb5Fndkr5E"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVIeR33iFqoV"
      },
      "source": [
        "Now that the data is ready, we will try to look at the shape of the data in a smaller dimension (equal to 2 in our case). For this, we use different dimension reduction techniques (TSNE, PCA and SVD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oglTeUSDdhL"
      },
      "source": [
        "red_tsne = TSNE(n_components=2,random_state=42).fit_transform(X_train)\n",
        "red_pca = PCA(n_components=2, random_state = 42).fit_transform(X_train)\n",
        "red_svd = TruncatedSVD(n_components = 2, algorithm = 'randomized', random_state = 42).fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbQuKQBhoA0a"
      },
      "source": [
        "row = np.random.rand(8000)\n",
        "df = pd.DataFrame(dict(row=row, nutriscore=y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BRZIJfhmJbz"
      },
      "source": [
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n",
        "f.suptitle('Clusters obtained by Dimensionality Reduction', fontsize=14)\n",
        "colors = {0:'red', 1:'green', 2:'blue',3:'yellow', 4: 'black'}\n",
        "ax1.scatter(red_tsne[:,0], red_tsne[:,1], c = df['nutriscore'].map(colors),  linewidths=2)\n",
        "ax1.set_title('t-SNE', fontsize=14)\n",
        "ax1.grid(True)\n",
        "ax2.scatter(red_pca[:,0], red_pca[:,1], c = df['nutriscore'].map(colors),  linewidths=2)\n",
        "ax2.set_title('PCA', fontsize=14)\n",
        "ax2.grid(True)\n",
        "ax3.scatter(red_svd[:,0], red_svd[:,1], c = df['nutriscore'].map(colors),  linewidths=2)\n",
        "ax3.set_title('Truncated SVD', fontsize=14)\n",
        "ax3.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd-AP6FdF1l3"
      },
      "source": [
        "**Note :** from the data form, we quickly realize that the problem is far from being easy. This is probably due to the surprises that take place during the manufacturing of a product. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QQ8foPDF1jb"
      },
      "source": [
        "In this step, we will use three algorithms:\n",
        "  - Two linear algorithms (Logistic Regression & Support Vector Classifier)\n",
        "  - The $K$-Nearest Neighbors Classifier Algorithm\n",
        "  - Two Ensemble learning  algorithms (Random Forest Classifier, Decision Tree Classifier)\n",
        "  - Two Gradient Boosting algorithms (LighGBM, XGBoost)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqHQvgGXwKU"
      },
      "source": [
        "train_accuracies = {'Logistic Regression':0, 'Support Vector Classifier':0, 'K-Neighbors Classifier':0, 'Random Forest Classifier':0, 'Decision Tree Classifier' : 0, \n",
        "                    'XGBoost Classifier' : 0,'lightgbm Classifier' : 0}\n",
        "test_accuracies = {'Logistic Regression':0, 'Support Vector Classifier':0, 'K-Neighbors Classifier':0, 'Random Forest Classifier':0, 'Decision Tree Classifier' : 0, \n",
        "                    'XGBoost Classifier' : 0,'lightgbm Classifier' : 0}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN9VxzY0dRB1"
      },
      "source": [
        "lgr = LogisticRegression()\n",
        "lgr.fit(X_train , y_train)\n",
        "train_preds = lgr.predict(X_train)\n",
        "test_preds = lgr.predict(X_test)\n",
        "scores1 = cross_val_score(lgr, train_preds.reshape(-1, 1), y_train.ravel(), scoring= 'accuracy', cv=10)\n",
        "scores2 = cross_val_score(lgr, test_preds.reshape(-1, 1), y_test.ravel(), scoring= 'accuracy', cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MawYbncFoyJ"
      },
      "source": [
        "print(\"Logistic Regression results :\")\n",
        "print(\"   -   Accuracy on the train set : {:.2f}%\".format(scores1.mean()*100))\n",
        "train_accuracies['Logistic Regression'] = scores1.mean()*100\n",
        "print(\"   -   Accuracy on the test set : {:.2f}%\".format(scores2.mean()*100))\n",
        "test_accuracies['Logistic Regression'] = scores2.mean()*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxIIjXzdamXK"
      },
      "source": [
        "svc = SVC()\n",
        "svc.fit(X_train , y_train)\n",
        "train_preds = svc.predict(X_train)\n",
        "test_preds = svc.predict(X_test)\n",
        "scores1 = cross_val_score(svc, train_preds.reshape(-1, 1), y_train.ravel(), scoring= 'accuracy', cv=10)\n",
        "scores2 = cross_val_score(svc, test_preds.reshape(-1, 1), y_test.ravel(), scoring= 'accuracy', cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0d3zTbEZR7m"
      },
      "source": [
        "print(\"Support Vector Classifier results :\")\n",
        "print(\"   -   Accuracy on the train set : {:.2f}%\".format(scores1.mean()*100))\n",
        "train_accuracies['Support Vector Classifier'] = scores1.mean()*100\n",
        "print(\"   -   Accuracy on the test set : {:.2f}%\".format(scores2.mean()*100))\n",
        "test_accuracies['Support Vector Classifier'] = scores2.mean()*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-idTS_J2xdy"
      },
      "source": [
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train , y_train)\n",
        "train_preds = knn.predict(X_train)\n",
        "test_preds = knn.predict(X_test)\n",
        "scores1 = cross_val_score(knn, train_preds.reshape(-1, 1), y_train.ravel(), scoring= 'accuracy', cv=10)\n",
        "scores2 = cross_val_score(knn, test_preds.reshape(-1, 1), y_test.ravel(), scoring= 'accuracy', cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NVjuljY24Ip"
      },
      "source": [
        "print(\"K-Neighbors Classifier results :\")\n",
        "print(\"   -   Accuracy on the train set : {:.2f}%\".format(scores1.mean()*100))\n",
        "train_accuracies['K-Neighbors Classifier'] = scores1.mean()*100\n",
        "print(\"   -   Accuracy on the test set : {:.2f}%\".format(scores2.mean()*100))\n",
        "test_accuracies['K-Neighbors Classifier'] = scores2.mean()*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iiZEmKaZSAu"
      },
      "source": [
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train , y_train)\n",
        "train_preds = dt.predict(X_train)\n",
        "test_preds = dt.predict(X_test)\n",
        "scores1 = cross_val_score(dt, train_preds.reshape(-1, 1), y_train.ravel(), scoring= 'accuracy', cv=10)\n",
        "scores2 = cross_val_score(dt, test_preds.reshape(-1, 1), y_test.ravel(), scoring= 'accuracy', cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGn-jd0O4oDH"
      },
      "source": [
        "print(\"Decision Tree Classifier results :\")\n",
        "print(\"   -   Accuracy on the train set : {:.2f}%\".format(scores1.mean()*100))\n",
        "train_accuracies['Decision Tree Classifier'] = scores1.mean()*100\n",
        "print(\"   -   Accuracy on the test set : {:.2f}%\".format(scores2.mean()*100))\n",
        "test_accuracies['Decision Tree Classifier'] = scores2.mean()*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOYM0SCm44s1"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train , y_train)\n",
        "train_preds = rf.predict(X_train)\n",
        "test_preds = rf.predict(X_test)\n",
        "scores1 = cross_val_score(rf, train_preds.reshape(-1, 1), y_train.ravel(), scoring= 'accuracy', cv=10)\n",
        "scores2 = cross_val_score(rf, test_preds.reshape(-1, 1), y_test.ravel(), scoring= 'accuracy', cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdzLCDkzZR_c"
      },
      "source": [
        "print(\"Random Forest Classifier results :\")\n",
        "print(\"   -   Accuracy on the train set : {:.2f}%\".format(scores1.mean()*100))\n",
        "train_accuracies['Random Forest Classifier'] = scores1.mean()*100\n",
        "print(\"   -   Accuracy on the test set : {:.2f}%\".format(scores2.mean()*100))\n",
        "test_accuracies['Random Forest Classifier'] = scores2.mean()*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "817aorppZR6F"
      },
      "source": [
        "gb = XGBClassifier()\n",
        "gb.fit(X_train , y_train)\n",
        "train_preds = gb.predict(X_train)\n",
        "test_preds = gb.predict(X_test)\n",
        "scores1 = cross_val_score(gb, X = train_preds.reshape(-1, 1), y = y_train.ravel(), scoring= 'accuracy', cv=10)\n",
        "scores2 = cross_val_score(gb, X =  test_preds.reshape(-1, 1), y = y_test.ravel(), scoring= 'accuracy', cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_kIyOpWgq6Q"
      },
      "source": [
        "print(\"XGBoost Classifier results :\")\n",
        "print(\"   -   Accuracy on the train set : {:.2f}%\".format(scores1.mean()*100))\n",
        "train_accuracies['XGBoost Classifier'] = scores1.mean()*100\n",
        "print(\"   -   Accuracy on the test set : {:.2f}%\".format(scores2.mean()*100))\n",
        "test_accuracies['XGBoost Classifier'] = scores2.mean()*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEJ-CU9lgq4s"
      },
      "source": [
        "gbm = lgb.LGBMClassifier()\n",
        "gbm.fit(X_train , y_train)\n",
        "train_preds = gbm.predict(X_train)\n",
        "test_preds = gbm.predict(X_test)\n",
        "scores1 = cross_val_score(gb, X = train_preds.reshape(-1, 1), y = y_train.ravel(), scoring= 'accuracy', cv=10)\n",
        "scores2 = cross_val_score(gb, X =  test_preds.reshape(-1, 1), y = y_test.ravel(), scoring= 'accuracy', cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1vBoNEVgq2H"
      },
      "source": [
        "print(\"XGBoost Classifier results :\")\n",
        "print(\"   -   Accuracy on the train set : {:.2f}%\".format(scores1.mean()*100))\n",
        "train_accuracies['lightgbm Classifier'] = scores1.mean()*100\n",
        "print(\"   -   Accuracy on the test set : {:.2f}%\".format(scores2.mean()*100))\n",
        "test_accuracies['lightgbm Classifier'] = scores2.mean()*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZxwEZmWgq0O"
      },
      "source": [
        "train_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h1xyLxY71ms"
      },
      "source": [
        "test_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4NMVRrVGXWM"
      },
      "source": [
        "ind = np.arange(7)\n",
        "width = 0.2\n",
        "\n",
        "fig = plt.figure(figsize=(14,8))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "rects1 = ax.bar(ind, list(train_accuracies.values()), width, color='b')\n",
        "rects2 = ax.bar(ind+width, list(test_accuracies.values()), width, color='g')\n",
        "\n",
        "ax.set_ylabel('Accuracy Score (%)')\n",
        "ax.set_xticks(ind+width)\n",
        "ax.set_xticklabels(list(test_accuracies.keys()) )\n",
        "ax.legend((rects1[0], rects2[0]), ('Train Accuracy', 'Test Accuracy'))\n",
        "\n",
        "plt.ylim((20,120))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4mWw_9tI2np"
      },
      "source": [
        "As we have seen from the data visualization, the **Ensemble Modeling algorithms**  are the most efficient although the data are far from being linearly separable. Moreover, **Random Forest** seems to be the most promising model in our case since it is the one that overfits less the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bK6KSknfBjn"
      },
      "source": [
        "y_pred = rf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CyHzUEJjYPi"
      },
      "source": [
        "output = pd.DataFrame({'Nutriscore_grade_Prediction': y_pred})\n",
        "\n",
        "filename = 'Predictions.csv'\n",
        "\n",
        "output.to_csv(filename,index=False)\n",
        "\n",
        "print('Saved file: ' + filename)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}